# ğŸ¤Ÿ ASL Sign Language Recognition API

A deep learning-powered FastAPI application for recognizing American Sign Language (ASL) hand gestures in real time from images. It uses a fine-tuned MobileNetV2 model trained on a dataset of 87,000 ASL gesture images across 29 classes (Aâ€“Z, SPACE, DELETE, NOTHING).

---

## ğŸ“‚ Dataset

- **Source**: [Kaggle - ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- **Size**: 87,000 training images, 29 classes (`A-Z`, `SPACE`, `DELETE`, `NOTHING`)
- **Image Size**: 200x200 pixels (resized to 224x224 for MobileNetV2)

---

## ğŸš€ Features

- âœ… FastAPI-powered web server with RESTful prediction endpoint
- âœ… Trained with MobileNetV2 + Dropout + BatchNorm
- âœ… Real-time inference support (via file upload)
- âœ… Real-time webcam-based prediction using OpenCV
- âœ… Returns predicted class and confidence
- âœ… Production-ready architecture
- âœ… Silent TensorFlow logs for clean output

---

## ğŸ§  Model

- ğŸ“š **Architecture**: MobileNetV2 (pretrained, fine-tuned)
- ğŸ¯ **Accuracy**: ~96% train accuracy, ~84% validation accuracy
- ğŸ’¾ Saved as: `asl_mobilenet_model.h5`

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/yourusername/asl-sign-predictor-api.git
cd asl-sign-predictor-api

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
````

---

## ğŸ“¦ Requirements

```
fastapi
uvicorn
tensorflow
opencv-python
numpy
python-multipart
Pillow
```

---

## ğŸš¦ Run the API

```bash
uvicorn main:app --reload
```

> Access Swagger UI: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§ª API Usage

### ğŸ” Endpoint: `POST /predict/`

#### âœ”ï¸ Parameters:

* `file`: image file (JPEG, PNG)

#### âœ”ï¸ Response:

```json
{
  "prediction": "G",
  "confidence": 0.987
}
```

#### âœ”ï¸ Example using `curl`:

```bash
curl -X POST "http://127.0.0.1:8000/predict/" \
     -F "file=@sample_asl.jpg"
```

---

## ğŸ¥ Real-Time Prediction

You can use your webcam or camera feed to predict ASL gestures live.

Run the script (e.g., `realtime.py`) with OpenCV to:

* Access webcam
* Capture frames
* Predict ASL class in real time
* Display prediction with confidence overlay

```bash
python realtime.py
```

> Make sure your model file `asl_mobilenet_model.h5` is available in the project root or updated in the script path.

---

## ğŸ–¼ï¸ Example Image Classes

| Class   | Meaning             |
| ------- | ------------------- |
| Aâ€“Z     | Alphabets           |
| SPACE   | Spacebar gesture    |
| DELETE  | Delete/backspace    |
| NOTHING | No gesture detected |

---

## ğŸ“ Project Structure

```
asl_api/
â”œâ”€â”€ main.py               # FastAPI app
â”œâ”€â”€ realtime.py           # Webcam-based real-time prediction
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ asl_mobilenet_model.h5
â””â”€â”€ README.md
```

---

## ğŸ“Œ To Do / Next Steps

* [ ] Add frontend (Streamlit or React)
* [ ] Deploy via Docker + Railway / Render / GCP
* [ ] Convert to TensorFlow Lite or ONNX

---

## ğŸ§‘â€ğŸ’» Author

**Iqrar Ali**
  Passionate ML Engineer
ğŸ“ Pakistan
ğŸ“§ [iqrarrajper22@gmail.com](mailto:iqrarrajper22@gmail.com)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/iqrar-ali-r-9a88a3214/)
---


